{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adfd7e93",
   "metadata": {},
   "source": [
    "This tutorial will demonstrate how to:\n",
    "- fit tuning curves given behavioral labels (e.g. position)\n",
    "- perform state-space decoding in the fashion of [Denovellis et. al. (2021)](https://elifesciences.org/articles/64505). It gives the posterior probability of the *label* and the *dynamics type* (they call it continuous and discrete variable, repsectively). \n",
    "    - *dynamics type* specifies the temporal prior of the label. When the dynamics type is *continuous*, the temporal prior of the label is a gaussian random walk, with a movement variance specified by the user. When the dynamics type is *fragmented*, the temporal prior of the label is uniform across all possible bins.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317d29a0",
   "metadata": {},
   "source": [
    "# import  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "134ef56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f267e10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import poor_man_gplvm as pmg\n",
    "import poor_man_gplvm.plot_helper as ph\n",
    "import pynapple as nap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4eebc3",
   "metadata": {},
   "source": [
    "# Load the data (ignore this section and replace with your own data)\n",
    "Some words on data preprocessing. We highly recommend [pynapple](https://pynapple.org/) as an entry point for neural data analysis in Python. They wrap around numpy objects but provide additional useful functionalities like restricting to time intervals, aligning to common time stamps, and turn spike times into counts. Essentially, for this tutorial, we need: \n",
    "- *spk_times*: pynapple TsGroup, obtained from a list of spike times (from the entire recording) for each unit.\n",
    "- *position_tsdf*: pynapple TsdFrame, obtained from an array of (n_time, n_columns), timestamps, and column names. Each column is one behavior label we will decode (doesn't have to be position).\n",
    "- *behavior_ep*: pynapple IntervalSet, obtained from arrays of start and end times of the behavior epoch when tuning curve is computed. \n",
    "- *speed_tsd* (optional): pynapple Tsd, obtained from an array of (n_time,) and timestamps. Here it is used for subselecting the locomotion epochs to include in the tuning curve computation. \n",
    "\n",
    "For the replay analysis later, we also need:\n",
    "- *ep_full*: pynapple IntervalSet, for the whole recording where replay will be identified \n",
    "- *ep_nrem*: pynapple IntervalSet, for the non-REM episodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee84f1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os\n",
    "sys.path.append('../../poor_gplvm/code')\n",
    "import preprocess_roman_tmaze as preprt\n",
    "\n",
    "\n",
    "\n",
    "data_dir_full = preprt.db_roman.iloc[0]['data_dir_full']\n",
    "\n",
    "prep_res = nap.load_folder(os.path.join(data_dir_full, \"derivatives\"))  \n",
    "\n",
    "\n",
    "spk_times = prep_res[\"spk_times\"]\n",
    "ripple_intervals = prep_res[\"ripple_intervals\"]\n",
    "position_tsdf = prep_res[\"position_tsdf\"]\n",
    "behavior_ep = prep_res['behavior_ep']\n",
    "\n",
    "\n",
    "behavior_ep = prep_res['behavior_ep']\n",
    "speed_tsd = prep_res['speed_tsd']\n",
    "\n",
    "# epochs for replay analysis\n",
    "ep_full = prep_res['full_ep']\n",
    "ep_nrem = prep_res['sleep_state_intervals_NREMepisode']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbadabc",
   "metadata": {},
   "source": [
    "# Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddd339c",
   "metadata": {},
   "source": [
    "## turn spike train into a matrix (TsdFrame, n_time x n_neuron) of spike counts \n",
    "Optional: use a mask to subselect only the pyramidal cells. This is easy if the relevant mask, e.g. *is_pyr* (whether it is a pyramidal cell) is stored as a metadata in the TsGroup.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6c82d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spk_times_pyr=spk_times[spk_times['is_pyr']]\n",
    "spk_mat = spk_times_pyr.count(0.1,ep=behavior_ep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3eed73",
   "metadata": {},
   "source": [
    "## prepare the labels and hyperparameters\n",
    "### labels\n",
    "In the paradigm of spatial navigation, the *label_l* can be time series of:\n",
    "- linearized positions \n",
    "- 2D positions \n",
    "- [choice port ID](https://www.nature.com/articles/s41586-024-08397-7)\n",
    "- linearized positions + direction\n",
    "- 2D positions + direction\n",
    "\n",
    "Indeed, in contrast to existing libraries of spatial decoding, we allow for arbitrary numbers of label dimension (up to memory constraint, so practically if one is already using 2D positions, the extra dimensions should not have too many discretized bins). \n",
    "\n",
    "Even for a linearizeable maze like the alternating T-maze, I personally still prefer using the 2D positions as labels. Whereas for a linear track, I would use the 1D position + direction, although 2D could give a subtler picture as hinted by [Zutshi et. al. (2025)](https://www.nature.com/articles/s41586-024-08397-7).\n",
    "\n",
    "\n",
    "### Hyperparameters\n",
    "- *label_bin_size*: binsize for discretizing the labels. \n",
    "- *smooth_std*: the standard deviation of the Gaussian kernel for smoothing the tuning curves. If None then no smoothing.\n",
    "- *occupancy_threshold*: the occupancy threshold (in seconds) for the label bin to be considered valid. \n",
    "\n",
    "All of the above can be either: 1) a single number that apply to all the dimensions; 2) a list of value per dimension; and 3) for multiple mazes, a dictionary of {maze_key: val}, where val can be 1) or 2).\n",
    "\n",
    "Here we will demonstrate the more general syntax assuming multiple mazes, but know that it can be simplified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77c36ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_d= {} \n",
    "label_d['familiar']=position_tsdf[['x','y']].restrict(behavior_ep[0]) # The restrict limit the x y coordinates to the first behavior epoch\n",
    "\n",
    "ep_d={}\n",
    "ep_d['familiar'] = speed_tsd.restrict(behavior_ep[0]).threshold(5).time_support\n",
    "\n",
    "label_bin_size_d = {}\n",
    "label_bin_size_d['familiar'] =  3.\n",
    "\n",
    "smooth_std_d = {}\n",
    "smooth_std_d['familiar'] = 3.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3e6b41",
   "metadata": {},
   "source": [
    "## below is if there's a second novel linear maze, with time_window given by `behavior_ep[1]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c884b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# novel_lin=position_tsdf[['lin']].restrict(behavior_ep[1])\n",
    "\n",
    "# novel_lin_dir = novel_lin.derivative() > 0\n",
    "\n",
    "# beh_tsdf_novel=nap.TsdFrame(d=np.stack([novel_lin.d,novel_lin_dir.d.astype(int)],axis=1),t=novel_lin.t,columns=['lin','dir'])\n",
    "\n",
    "# label_d['novel'] = beh_tsdf_novel\n",
    "\n",
    "# speed_tsd_novel = np.abs(novel_lin.derivative())\n",
    "\n",
    "# ep_d['novel'] = speed_tsd_novel.threshold(5).time_support\n",
    "\n",
    "\n",
    "\n",
    "# label_bin_size_d['novel'] = [3.,1.]\n",
    "\n",
    "# smooth_std_d['novel'] = [3,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1428ae44",
   "metadata": {},
   "source": [
    "# compute tuning curves\n",
    "Tuning curves are computed by: 1) discretize the multi-dimensional labels into bins; 2) get occupancy of each bin; 3) drop the low occupancy bins; 4) compute the spike counts emitted within each bin; 5) smooth the occupancy and spike counts using a Gaussian kernel; 6) FR = count_smoothed / occupancy_smoothed, in Hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24a5d863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'poor_man_gplvm' from '/mnt/home/szheng/projects/poor-man-GPLVM/poor_man_gplvm/__init__.py'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "reload(pmg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79bb4528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[get_tuning] maze=familiar: grid_shape=(29, 28), n_neurons=422, dt=0.1000s, n_valid_time=8930, n_occupied_bins=306\n"
     ]
    }
   ],
   "source": [
    "tuning_res = pmg.get_tuning_supervised(\n",
    "    spk_mat,# nap.TsdFrame, n_time x n_neuron\n",
    "    label_d,\n",
    "    ep=ep_d,                   # nap.IntervalSet (optional)\n",
    "    label_bin_size=label_bin_size_d,          # cm\n",
    "    smooth_std=smooth_std_d,              # cm, Gaussian kernel std\n",
    "    occupancy_threshold=0.1,     # seconds\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf884e14",
   "metadata": {},
   "source": [
    "# Replay analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85209b11",
   "metadata": {},
   "source": [
    "## preprocessing for replay analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55070ac9",
   "metadata": {},
   "source": [
    "### Identify population burst events\n",
    "We bin spikes finely (`bin_size`,default 2ms), average across neurons (pyramidal cells), lightly smooth (`smooth_std`, default 7.5ms), z-score, identify the windows where the firing rate reachs beyond 3 standard deviations `z_thresh`, and extend both sides of the window back to the mean. The events are filtered by a `min_duration` and `max_duration`.\n",
    "\n",
    "- `ep`: pynapple IntervalSet, time epochs where the population burst events are to be identified\n",
    "- `threshold_ep`: pynapple IntervalSet, time epochs where the mean and standard deviation of firing rates are computed. Here we choose only the non-REM epochs.\n",
    "- `force_reload`: whether to re-compute if the file already exists\n",
    "- `dosave`: whether to save\n",
    "- `return_population_rate`: whether to return the population firing rate, since it can be a long time series\n",
    "\n",
    "Can optionally supply `ripple_intervals` (pynapple IntervalSet) so that the detected PBE is aware of how many sharp wave ripples are contained within. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf8e936a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/home/szheng/ceph/ad/roman_data/e13/e13_26m1/e13_26m1_210913/py_data/pbe.p exists; loading---\n",
      "CPU times: user 97.1 ms, sys: 6.88 ms, total: 104 ms\n",
      "Wall time: 150 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# get pbe\n",
    "import poor_man_gplvm.post_fit_workflow.get_event_windows as gew\n",
    "\n",
    "is_pyr = spk_times['is_pyr'] # assuming is_pyr is in the metadata of spk_times; otherwise a boolean array of length n_neuron would work\n",
    "spk_times_pyr=spk_times[is_pyr] \n",
    "pbe_save_dir=os.path.join(data_dir_full,'py_data') # replace with the directory you like!\n",
    "save_fn='pbe.p'#'pbe_wake_and_sleep_all.p'#\n",
    "\n",
    "\n",
    "pbe_res=gew.detect_population_burst_event(spk_times_pyr, mask=None, ep=ep_full,threshold_ep=prep_res['sleep_state_intervals_NREMepisode'], bin_size=0.002, smooth_std=0.0075, \n",
    "                                 z_thresh=3.0, min_duration=0.05, max_duration=0.5,\n",
    "                                 ripple_intervals=None,force_reload=False,dosave=False,save_dir=pbe_save_dir,return_population_rate=False,save_fn=save_fn)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf6549e",
   "metadata": {},
   "source": [
    "### Bin spikes\n",
    "To work with data grouped by events / trials, we use a helper function that takes in `spk_times` (pynapple TsGroup) and `binsize` (default 0.02 s) and bin the spikes into two formats: `spike_mat_padded` and `spike_mat`.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27011276",
   "metadata": {},
   "outputs": [],
   "source": [
    "import poor_man_gplvm.trial_analysis as tri\n",
    "binsize = 0.02\n",
    "spk_tensor_res=tri.bin_spike_train_to_trial_based(spk_times,pbe_res['event_windows'],binsize=binsize)\n",
    "\n",
    "spk_tensor=spk_tensor_res['spike_mat_padded']\n",
    "spk_mat_pbe=spk_tensor_res['spike_mat']\n",
    "tensor_pad_mask=spk_tensor_res['mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2058843d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor format shape (n_event, n_time_max, n_neuron) (5400, 25, 482)\n",
      "Matrix format shape (n_time_concat, n_neuron) (30532, 482)\n",
      "Tensor pad mask shape (n_event, n_time_max, 1) (5400, 25, 1)\n"
     ]
    }
   ],
   "source": [
    "print('Tensor format shape (n_event, n_time_max, n_neuron)',spk_tensor.shape)\n",
    "print('Matrix format shape (n_time_concat, n_neuron)',spk_mat_pbe.shape)\n",
    "print('Tensor pad mask shape (n_event, n_time_max, 1)',tensor_pad_mask.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389249d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b1d28b3",
   "metadata": {},
   "source": [
    "# First question - whether an event is better-explained by the tuning templates than chance (goodness-of-fit, \"on-manifold\"ness)\n",
    "For the first question we measure the model fit by the marginal likelihood of the Naive Bayes decoder. There is no temporal prior. It is asking whether the population vectors lie on the manifold of tuning curves, ignoring any question about temporal sequenceness. \n",
    "\n",
    "We use two shuffles to answer this question:\n",
    "- *Neuron ID shuffle*: check whether the Neuron - tuning curve association is needed for the model fit\n",
    "- *Circular shuffle*: check whether the **co-activity** is driving the model fit (rather than single neuron firing statistics)\n",
    "\n",
    "Usually the *circular shuffle* test is harder to pass. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de869eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import poor_man_gplvm.post_fit_workflow.shuffle_test_decoding as shuf\n",
    "\n",
    "dt = 0.02\n",
    "gain = 1\n",
    "flat_idx_to_coord = tuning_res['flat_idx_to_coord']\n",
    "event_index_per_bin = spk_tensor_res['event_index_per_bin']\n",
    "spk_mat_pbe = spk_tensor_res['spike_mat']\n",
    "\n",
    "shuffle_res = shuf.shuffle_test_naive_bayes_marginal_l(\n",
    "    spk_mat_pbe,\n",
    "    event_index_per_bin,\n",
    "    tuning=tuning_res['tuning_flat'],\n",
    "    n_shuffle=100,\n",
    "    sig_thresh=0.95,\n",
    "    seed=0,\n",
    "    decoding_kwargs={'dt': dt, 'gain': gain, 'n_time_per_chunk': 20000},\n",
    "    dosave=True,\n",
    "    force_reload=False,\n",
    "    save_dir='./',\n",
    "    save_fn='test_shuffle.p',\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jaxnew2",
   "language": "python",
   "name": "jaxnew2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
